{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-odEg64EKVXO"
      },
      "source": [
        "# HW 1 - Policy Gradients & Proximal Policy Optimization\n",
        "This assignment builds to a simple PPO (2017) implementations by progressing from PPOs predecessor algorithms: <br> REINFORCE (\\~1992) and Vanilla Policy Gradients (\\~1999). Note, many variations of these algorithms exist. Please use the math contained in this notebook for the coding sections.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YkZLMuXu7Fm"
      },
      "source": [
        "## 0. Warm Up Questions [30 pts total; 2 pt each]\n",
        "Answer each question concisely. One sentence, one formula, one line of code, etc. Use of $\\LaTeX$ formatting for math is encouraged.\n",
        "\n",
        "1.    What is an MDP and what are its four main parts? <br>\n",
        "Type answer here ...\n",
        "\n",
        "2.    What is the markov property? <br>\n",
        "\n",
        "\n",
        "3.    What is the formula for the objective aka sum of discounted rewards? <br>\n",
        "\n",
        "\n",
        "4.    Complete the sentence: 'Policy gradient' is shorthand for the 'gradient of ??? with respect to ???'. <br>\n",
        "\n",
        "\n",
        "5.    What does $\\nabla _\\theta J (\\theta)$ mean in basic language? <br>\n",
        "\n",
        "\n",
        "6.    What is the formula for gradient of the objective in REINFORCE? (policy gradient slides - Canvas/files/lec-4)\n",
        "\n",
        "$$ \\nabla _\\theta J (\\theta) \\approx  ???$$\n",
        "\n",
        "\n",
        "7.    Does subtracting a baseline from returns introduce bias in expectation? (policy gradient slides - Canvas/files/lec-4) <br>\n",
        "\n",
        "8.    Do on-policy algorithms use a replay buffer? <br>\n",
        "\n",
        "9.    What does $\\pi _\\theta (a_t | s_t)$ mean in basic language? <br>\n",
        "\n",
        "10.    What is the log prob of getting heads when flipping a coin? <br>\n",
        "\n",
        "11.    Finish this basic property of logs:\n",
        "$\\frac{A}{B} = \\exp (\\log A - ? )$\n",
        "\n",
        "12. What is a logit in the DRL context? <br>\n",
        "\n",
        "13. Is a Categorical Distribution continuous or discrete? <br>\n",
        "\n",
        "14. Logits are used to construct a Categorical distribution. Finish the code to get the log probability of the actions that were sampled. Hint: https://pytorch.org/docs/stable/distributions.html\n",
        "\n",
        "        logits = self.policy(obs)\n",
        "        probs = categorical.Categorical(logits=logits)\n",
        "        actions = probs.sample()\n",
        "        log_probs = _____\n",
        "\n",
        "\n",
        "15. In [CartPole-v1](https://www.gymlibrary.dev/environments/classic_control/cart_pole/) what are the physical meanings of states and actions and are they discrete or continuous?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2ltUIFtSIRo"
      },
      "source": [
        "## Imports and Set up\n",
        "Installs gymnasium, imports deep learning libs, sets torch device. **You shouldnt need to change this code.** Your colab runtime should default to CPU. To double check: **click Runtime (top left of notebook) -> Change runtime type -> select a CPU -> Save**. For simplicity, this notebook doesnt manage data transfers between CPU and GPU. You need to use CPU runtime for it to work unmodified. Feel free to experiment with GPUs after submitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "j4bFtFg6LITX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
            "Requirement already satisfied: gymnasium in /home/jblevins32/anaconda3/lib/python3.11/site-packages (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /home/jblevins32/anaconda3/lib/python3.11/site-packages (from gymnasium) (1.24.3)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /home/jblevins32/anaconda3/lib/python3.11/site-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /home/jblevins32/anaconda3/lib/python3.11/site-packages (from gymnasium) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /home/jblevins32/anaconda3/lib/python3.11/site-packages (from gymnasium) (0.0.4)\n",
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium\n",
        "import gymnasium as gym\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim import Adam\n",
        "from torch.distributions import categorical\n",
        "from copy import deepcopy\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import random\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# random seeds for reproducability\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed_all(0)\n",
        "random.seed(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "cellView": "form",
        "id": "3LnvUng8oQsQ"
      },
      "outputs": [
        {
          "ename": "AssertionError",
          "evalue": "Test failed: Device is not CPU! Read Imports and Set up",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[12], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m device\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest failed: Device is not CPU! Read Imports and Set up\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Run the test\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m test_device_is_cpu()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest passed: Device is CPU.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[0;32mIn[12], line 3\u001b[0m, in \u001b[0;36mtest_device_is_cpu\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest_device_is_cpu\u001b[39m():\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m device\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest failed: Device is not CPU! Read Imports and Set up\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
            "\u001b[0;31mAssertionError\u001b[0m: Test failed: Device is not CPU! Read Imports and Set up"
          ]
        }
      ],
      "source": [
        "#@title Device check\n",
        "def test_device_is_cpu():\n",
        "    assert device.type == \"cpu\", \"Test failed: Device is not CPU! Read Imports and Set up\"\n",
        "\n",
        "# Run the test\n",
        "test_device_is_cpu()\n",
        "print(\"Test passed: Device is CPU.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2Cd4Pf9SkM9"
      },
      "source": [
        "## Trajectory Data Storage\n",
        "This is boiler plate code that lets your on-policy algorithms store their interactions with the environment. It also calculates returns as the sum of discounted rewards\n",
        "$\n",
        "R = \\sum_{t=0}^{T} \\gamma^t r_t\n",
        "$.\n",
        "Note, when a terminal condition is reached (not_dones = False), the sum resets to 0. More sopisticated PPO implementations use GAE, but it will work without it. **You shouldn't need to change this code,** but you should understand the store() and calc_returns() functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "65bvpyHhL7Jw"
      },
      "outputs": [],
      "source": [
        "class TrajData:\n",
        "    def __init__(self, n_steps, n_envs, n_obs, n_actions):\n",
        "        s, e, o, a = n_steps, n_envs, n_obs, n_actions\n",
        "        from torch import zeros\n",
        "\n",
        "        self.states = zeros((s, e, o))\n",
        "        self.actions = zeros((s, e))\n",
        "        self.rewards = zeros((s, e))\n",
        "        self.not_dones = zeros((s, e))\n",
        "\n",
        "        self.log_probs = zeros((s, e))\n",
        "        self.returns = zeros((s, e))\n",
        "\n",
        "        self.n_steps = s\n",
        "\n",
        "    def detach(self):\n",
        "        self.actions = self.actions.detach()\n",
        "        self.log_probs = self.log_probs.detach()\n",
        "\n",
        "    def store(self, t, s, a, r, lp, d):\n",
        "        self.states[t] = s\n",
        "        self.actions[t] = a\n",
        "        self.rewards[t] = torch.Tensor(r)\n",
        "\n",
        "        self.log_probs[t] = lp\n",
        "        self.not_dones[t] = 1 - torch.Tensor(d)\n",
        "\n",
        "    def calc_returns(self, gamma = .99):\n",
        "        self.returns = deepcopy(self.rewards)\n",
        "\n",
        "        for t in reversed(range(self.n_steps-1)):\n",
        "            self.returns[t] += self.returns[t+1] * self.not_dones[t] * gamma"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3I2uUuD3YS_I"
      },
      "source": [
        "## DRL Rollout and Update Loop\n",
        "This is more boiler plate code. It instantiates your parallel gym environments, neural nets (which you will define next), optimizer, and tensorboard logging. It also establishes the rollout/update cycle. During rollout, the agent collects $(s, a, r)$ tuples from the environment. During update, losses are calculated and the DRL agent is updated via gradient descent. **You shouldn't need to change this code.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "yOmLqtPSL9E_"
      },
      "outputs": [],
      "source": [
        "class DRL:\n",
        "    def __init__(self):\n",
        "\n",
        "        self.n_envs = 64\n",
        "        self.n_steps = 256\n",
        "        self.n_obs = 4\n",
        "\n",
        "        self.envs = gym.vector.SyncVectorEnv([lambda: gym.make(\"CartPole-v1\") for _ in range(self.n_envs)])\n",
        "\n",
        "        self.traj_data = TrajData(self.n_steps, self.n_envs, self.n_obs, n_actions=1) # 1 action choice is made\n",
        "        self.agent = Agent(self.n_obs, n_actions=2)  # 2 action choices are available\n",
        "        self.optimizer = Adam(self.agent.parameters(), lr=1e-3)\n",
        "        self.writer = SummaryWriter(log_dir=f'runs/{self.agent.name}')\n",
        "\n",
        "\n",
        "    def rollout(self, i):\n",
        "\n",
        "        obs, _ = self.envs.reset()\n",
        "        obs = torch.Tensor(obs)\n",
        "\n",
        "        for t in range(self.n_steps):\n",
        "            # PPO doesnt use gradients here, but REINFORCE and VPG do.\n",
        "            with torch.no_grad() if self.agent.name == 'PPO' else torch.enable_grad():\n",
        "                actions, probs = self.agent.get_action(obs)\n",
        "            log_probs = probs.log_prob(actions)\n",
        "            next_obs, rewards, done, truncated, infos = self.envs.step(actions.numpy())\n",
        "            done = done | truncated  # episode doesnt truncate till t = 500, so never\n",
        "            self.traj_data.store(t, obs, actions, rewards, log_probs, done)\n",
        "            obs = torch.Tensor(next_obs)\n",
        "\n",
        "        self.traj_data.calc_returns()\n",
        "\n",
        "        self.writer.add_scalar(\"Reward\", self.traj_data.rewards.mean(), i)\n",
        "        self.writer.flush()\n",
        "\n",
        "\n",
        "    def update(self):\n",
        "\n",
        "        # A primary benefit of PPO is that it can train for\n",
        "        # many epochs on 1 rollout without going unstable\n",
        "        epochs = 10 if self.agent.name == 'PPO' else 1\n",
        "\n",
        "        for _ in range(epochs):\n",
        "\n",
        "            loss = self.agent.get_loss(self.traj_data)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "        self.traj_data.detach()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTWYOKF-bxFa"
      },
      "source": [
        "## Tensorboard\n",
        "This will launch an interactive tensorboard window within collab. It will display rewards in (close to) real time while your agents are training. You'll likely have to refresh if its not updating (circular arrow to right in the orange bar). **You shouldn't need to change this code.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "5iKKG2ZwM9hW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Reusing TensorBoard on port 6006 (pid 56989), started 0:01:40 ago. (Use '!kill 56989' to kill it.)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "      <iframe id=\"tensorboard-frame-629f6fbed82c07cd\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
              "      </iframe>\n",
              "      <script>\n",
              "        (function() {\n",
              "          const frame = document.getElementById(\"tensorboard-frame-629f6fbed82c07cd\");\n",
              "          const url = new URL(\"http://localhost\");\n",
              "          const port = 6006;\n",
              "          if (port) {\n",
              "            url.port = port;\n",
              "          }\n",
              "          frame.src = url;\n",
              "        })();\n",
              "      </script>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Launch TensorBoard\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "cellView": "form",
        "id": "RKOqilTFO_t9"
      },
      "outputs": [],
      "source": [
        "# @title Visualization code. Used later.\n",
        "\n",
        "import os\n",
        "from gym.wrappers import RecordVideo\n",
        "from IPython.display import Video, display, clear_output\n",
        "\n",
        "def visualize(agent):\n",
        "\n",
        "    video_dir = \"./videos\"  # Directory to save videos\n",
        "    os.makedirs(video_dir, exist_ok=True)\n",
        "\n",
        "    # Create environment with proper render_mode\n",
        "    env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
        "\n",
        "    # Apply video recording wrapper\n",
        "    env = RecordVideo(env, video_folder=video_dir, episode_trigger=lambda x: True)\n",
        "\n",
        "    obs, _ = env.reset()\n",
        "\n",
        "\n",
        "    for t in range(4096):\n",
        "        actions, _ = agent.get_action(torch.Tensor(obs)[None, :])  # Get action from policy\n",
        "        obs, _, done, _ = env.step(actions.cpu().item())\n",
        "\n",
        "        if done:\n",
        "            # self.writer.add_scalar(\"Duration\", t, i)\n",
        "            break\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    # Display the latest video\n",
        "    video_path = os.path.join(video_dir, sorted(os.listdir(video_dir))[-1])  # Get the latest video\n",
        "\n",
        "\n",
        "    clear_output(wait=True)\n",
        "    display(Video(video_path, embed=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hs6mPduOcQ0T"
      },
      "source": [
        "--------------------------------------------------------------------------------\n",
        "## 1. REINFORCE [30 pts]\n",
        "1.   Define your policy network [10 pts]\n",
        "2.   Define the reinforce policy loss using rollout data stored in traj_data [15 pts]\n",
        "3.   Conceptual question [5 pts]\n",
        "\n",
        "--------------------------------------------------------------------------------\n",
        "HINTS:\n",
        "\n",
        "If you're not super familar with defining networks in pytorch, check out this [tutorial](https://medium.com/writeasilearn/using-sequential-module-to-build-a-neural-network-a34ca3f37203).\n",
        "\n",
        "#### Policy loss for REINFORCE:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(\\theta) = -\\frac{1}{N \\cdot T} \\sum_{i=0}^N \\sum_{t=0}^T \\log \\pi_\\theta(a_{i,t} | s_{i,t}) \\cdot R_{i,t}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $ \\mathcal{L} $ is the policy loss; a function of network parameters $\\theta$\n",
        "- $N$ is the total number of environments\n",
        "- $T$ is the total number of time steps (the slides don't divide by $T$, but it doesnt change the gradient, and you need it to pass the unit tests)\n",
        "- $ \\log \\pi_{\\theta}(a_{i, t} | s_{i,t}) $ is the logarithm of the probability of the action $a$ that was taken in state $s$, given policy $\\pi$ parametrized by $\\theta$, at timestep t in environment i\n",
        "- $ R_{i,t} $ is the return (sum of discounted rewards) for environment i at timestep t\n",
        "\n",
        "<br>\n",
        "\n",
        "For simplicity, expectation notation is often used, and the subscript $i$ is often dropped:\n",
        "$$\n",
        "\\mathcal{L}(\\theta) = -\\mathbb{E}\\left[ \\log \\pi_{\\theta}(a_t | s_t) \\cdot R_t \\right]\n",
        "$$\n",
        "\n",
        "We follow this convention going forward.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "z_a2Ji6KLpYo"
      },
      "outputs": [],
      "source": [
        "class Agent(nn.Module):\n",
        "    def __init__(self, n_obs, n_actions):  # use these\n",
        "        super().__init__()\n",
        "        self.name = 'REINFORCE'\n",
        "\n",
        "        torch.manual_seed(0)  # needed before policy init for fair comparison\n",
        "\n",
        "        # todo: student code here\n",
        "        self.policy = nn.Sequential(\n",
        "            nn.Linear(n_obs, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, n_actions),\n",
        "        ) \n",
        "        # end student code\n",
        "\n",
        "    def get_loss(self, traj_data):\n",
        "        # todo: student code here\n",
        "\n",
        "        # Multiplying each return by its respective log probability, summing it all together, and dividing by the number of steps and time\n",
        "        policy_loss = -torch.sum(traj_data.log_probs * traj_data.returns)/(traj_data.n_steps*traj_data.returns.shape[1])\n",
        "\n",
        "        # end student code\n",
        "        return policy_loss\n",
        "\n",
        "    def get_action(self, obs):\n",
        "        logits = self.policy(obs)\n",
        "        probs = categorical.Categorical(logits=logits)\n",
        "        actions = probs.sample()\n",
        "        return actions, probs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "cellView": "form",
        "id": "T884A8vHUZwz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test passed: REINFORCE policy appears correct!\n",
            "Test passed: REINFORCE loss appears correct!\n"
          ]
        }
      ],
      "source": [
        "# @title REINFORCE Unit Tests (must run REINFORCE Agent cell above first)\n",
        "def REINFORCE_policy():\n",
        "    a = Agent(16, 4)\n",
        "    assert a.name == 'REINFORCE' and \\\n",
        "    a.policy(torch.randn(8, 16)).shape == (8, 4) and \\\n",
        "    isinstance(list(a.policy.children())[-1], nn.Linear), \\\n",
        "    f\"Network not initialized correctly\"\n",
        "    print(\"Test passed: REINFORCE policy appears correct!\")\n",
        "\n",
        "REINFORCE_policy()\n",
        "\n",
        "def REINFORCE_loss():\n",
        "    n_steps, n_envs, n_obs, n_actions = 10, 5, 4, 1\n",
        "    traj_data = TrajData(n_steps, n_envs, n_obs, n_actions)\n",
        "    torch.manual_seed(0)\n",
        "    traj_data.states = torch.rand_like(traj_data.states)\n",
        "    traj_data.actions = torch.randint(0, n_actions, traj_data.actions.shape)\n",
        "    traj_data.rewards = torch.rand_like(traj_data.rewards)\n",
        "    traj_data.not_dones = torch.randint(0, 2, traj_data.not_dones.shape)\n",
        "    traj_data.log_probs = torch.rand_like(traj_data.log_probs)\n",
        "    traj_data.returns = torch.rand_like(traj_data.returns)\n",
        "    a = Agent(n_obs=n_obs, n_actions=n_actions)\n",
        "    assert abs(a.get_loss(traj_data).item() - (-0.2369)) < 1e-4, \\\n",
        "    \"REINFORCE loss does not match expected value.\"\n",
        "    print(\"Test passed: REINFORCE loss appears correct!\")\n",
        "\n",
        "REINFORCE_loss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvZxxF6TLrP0"
      },
      "source": [
        "Run the REINFORCE Agent cell above, and then run the rollout/update cell below. <br> Scroll back up to tensorboard and refresh (circular white arrow in the right of the orange bar) to visualize your reward curve.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "BleZWK8yMFyp"
      },
      "outputs": [],
      "source": [
        "drl = DRL()\n",
        "for i in range(250):\n",
        "    drl.rollout(i)\n",
        "    drl.update()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XQsYbKC3qas"
      },
      "source": [
        "#### REINFORCE Conceptual question:\n",
        "In 1 or 2 sentences, how does minimizing the REINFORCE loss above achieve our RL goal? <br> Hint: (policy gradient slides - Canvas/files/lec-4 - \"What did we just do?\")<br>\n",
        "\n",
        "Minimizing the REINFORCE loss allows us to update our policy accordingly, thus maximizing the probability of choosing actions that provide the highest cumulative reward."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-P2zM8Bnc6d"
      },
      "source": [
        "--------------------------------------------------------------------------------\n",
        "## 2. Vanilla Policy Gradient (aka REINFORCE with Baseline)[40 pts]\n",
        "\n",
        "1.   Define your networks [15 pts]\n",
        "  *   Value network\n",
        "  *   Policy network (same as before)\n",
        "\n",
        "\n",
        "2.   Define your losses [20 pts]\n",
        "  *   Value loss\n",
        "  *   Policy loss (similar to before)\n",
        "  *   Add them\n",
        "\n",
        "3.   Conceptual question [5 pts]\n",
        "--------------------------------------------------------------------------------\n",
        "HINTS:\n",
        "\n",
        "#### Value loss\n",
        "Mean Squared Error (MSE) between the experienced returns and predicted value:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_{\\text{value}}(\\theta) = \\mathbb{E}\\left[ (R_t - V_{\\theta}(s_t))^2 \\right]\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $ \\mathcal{L}_{\\text{value}}(\\theta) $ is the value network loss\n",
        "- $ V_{\\theta}(s_t) $ is the predicted value for state $ s_t $ from the value network\n",
        "- $ R_t $ is the return (sum of discounted rewards)\n",
        "\n",
        "\n",
        "#### Policy Loss\n",
        "\n",
        "The VPG policy loss is quite similar to REINFORCE, but rather than using returns, we use returns minus a baseline value prediction. This quantity is known as the advantage $A(s_t, a_t)$. The advantage is usually defined as $A(s_t, a_t) = Q(s_t, a_t) - V(s_t)$. Returns act as the Q function in our case.\n",
        "\n",
        "$$A(s_t, a_t) = R_t - V_{\\theta}(s_t)$$\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_{\\text{policy}}(\\theta) = - \\mathbb{E}\\left[ \\log \\pi_{\\theta}(a_t | s_t) \\cdot A(s_t, a_t) \\right]\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $ \\mathcal{L}_{\\text{policy}}(\\theta) $ is the policy loss\n",
        "- $ \\log \\pi_{\\theta}(a_t | s_t) $ is the logarithm of the probability of the action $a_t$ that was taken in state $s_t$\n",
        "- $A(s_t, a_t)$ is the advantage of action $a_t$ that was taken in $s_t$, campared to the average for state $s_t$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8GEi3NvqUTD"
      },
      "outputs": [],
      "source": [
        "class Agent(nn.Module):\n",
        "    def __init__(self, n_obs, n_actions):  # use these\n",
        "        super().__init__()\n",
        "        self.name = 'VPG'\n",
        "\n",
        "        torch.manual_seed(0)  # needed before network init for fair comparison\n",
        "\n",
        "        # todo: student code here\n",
        "        self.policy = None  # replace\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        self.value = None  # replace\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # end student code\n",
        "\n",
        "    def get_loss(self, traj_data):\n",
        "\n",
        "\n",
        "        # todo: student code here\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        loss = None  # replace\n",
        "        # end student code\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def get_action(self, obs):\n",
        "        logits = self.policy(obs)\n",
        "        probs = categorical.Categorical(logits=logits)\n",
        "        actions = probs.sample()\n",
        "        return actions, probs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0T0KKk6jlTS"
      },
      "outputs": [],
      "source": [
        "# @title VPG Units Tests (must run VPG Agent cell above first)\n",
        "def VPG_networks():\n",
        "    a = Agent(32, 6)\n",
        "    assert a.name == 'VPG' and \\\n",
        "    a.policy(torch.randn(64, 32)).shape == (64, 6) and \\\n",
        "    a.value(torch.randn(64, 32)).shape == (64, 1) and \\\n",
        "    isinstance(list(a.policy.children())[-1], nn.Linear), \\\n",
        "    f\"Networks not initialized correctly\"\n",
        "    print(\"Test passed: VPG Networks appear correct!\")\n",
        "\n",
        "VPG_networks()\n",
        "\n",
        "def VPG_loss():\n",
        "    n_steps, n_envs, n_obs, n_actions = 10, 5, 4, 1\n",
        "    traj_data = TrajData(n_steps, n_envs, n_obs, n_actions)\n",
        "    torch.manual_seed(0)\n",
        "    traj_data.states = torch.rand_like(traj_data.states)\n",
        "    traj_data.actions = torch.randint(0, n_actions, traj_data.actions.shape)\n",
        "    traj_data.rewards = torch.rand_like(traj_data.rewards)\n",
        "    traj_data.not_dones = torch.randint(0, 2, traj_data.not_dones.shape)\n",
        "    traj_data.log_probs = torch.rand_like(traj_data.log_probs)\n",
        "    traj_data.returns = torch.rand_like(traj_data.returns)\n",
        "    a = Agent(4, 1)\n",
        "    torch.manual_seed(0)\n",
        "    a.policy = nn.Linear(4, 1)\n",
        "    a.value = nn.Linear(4, 1)\n",
        "    assert abs(a.get_loss(traj_data).item() - 0.0618) < 1e-4, \\\n",
        "    \"VPG loss does not match expected value.\"\n",
        "    print(\"Test passed: VPG loss appears correct!\")\n",
        "\n",
        "VPG_loss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8WVzb5BMGu2"
      },
      "source": [
        "Run the VPG Agent cell above, and then run the rollout/update cell below. <br> Scroll back up to tensorboard and refresh (circular white arrow in the right of the orange bar) to visualize your reward curve."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WnAiItxlrJ7Q"
      },
      "outputs": [],
      "source": [
        "drl = DRL()\n",
        "for i in range(250):\n",
        "    drl.rollout(i)\n",
        "    drl.update()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtgMGojY6Qsb"
      },
      "source": [
        "#### VPG Conceptual Question:\n",
        "In 2 or 3 sentences, why might subtracting a value network baseline improve performance of our RL agent? (Hint: policy gradient slides) Based on the tensorboard curves, what is the effect in this environment? Why? <br>\n",
        "\n",
        "Type answer here..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbzNmNLMMFTj"
      },
      "source": [
        "--------------------------------------------------------------------------------\n",
        "## 3. Optional Extra Credit: Proximal Policy Optimization <br> (aka REINFORCE with Baseline and Clipped Surrogate Objective) [10pts]\n",
        "\n",
        "1.   Define your networks [1 pts]\n",
        "  *   Value network (same as VPG)\n",
        "  *   Policy network (same as VPG)\n",
        "\n",
        "2.   Define your losses [5 pts]\n",
        "  *   Value loss (same as VPG)\n",
        "  *   Policy loss (the heart of PPO)\n",
        "  *   Add them\n",
        "\n",
        "3. Generalized Advantage Estimation (GAE) [4 pts]\n",
        "--------------------------------------------------------------------------------\n",
        "HINTS:\n",
        "\n",
        "#### Policy Loss\n",
        "\n",
        "Our PPO policy loss still uses the advantage defined in VPG:$$A(s_t, a_t)  = A_t = R_t - V_{\\theta}(s_t)$$\n",
        "\n",
        "But we maximize a clipped surrogate objective which is designed to keep policy updates bounded:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_{\\text{clip}}(\\theta) = \\mathbb{E}_t \\left[ \\min \\left( \\frac{\\pi_\\theta(a_t | s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t | s_t)} \\cdot A_t, \\text{clip}\\left(\\frac{\\pi_\\theta(a_t | s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t | s_t)}, 1 - \\epsilon, 1 + \\epsilon\\right) \\cdot A_t \\right) \\right].\n",
        "$$\n",
        "\n",
        "where:\n",
        "\n",
        "- $ \\pi_\\theta(a_t | s_t) $: the probability of taking action $a_t$ in state $s_t$ under the current policy with parameters $ \\theta $.\n",
        "- $ \\pi_{\\theta_{\\text{old}}}(a_t | s_t) $: the probability of taking action $a_t$ in state $s_t$ under the old policy before the update.\n",
        "- $ A_t $: the advantage estimate at timestep $t$.\n",
        "- $ \\epsilon $: the clip range hyperparameter that limits policy updates.\n",
        "- $ \\text{clip}(x, 1 - \\epsilon, 1 + \\epsilon) $: clips $x$ to the range $[1 - \\epsilon, 1 + \\epsilon]$ to ensure conservative updates.\n",
        "\n",
        "<br>\n",
        "\n",
        "Lets break it down.\n",
        "\n",
        "\n",
        "*   First, $\\frac{\\pi_\\theta(a_t | s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t | s_t)}$ is the probability ratio between the policy being updated and the policy that was rolled out to collect the training data (traj_data). It is only meaningful when multiple epochs of training are performed on the training data from a single rollout. Indeed, in the first epoch, the current and old policies are the same so the ratio will be one.\n",
        "\n",
        "*   For numerical stability, we leverage a basic property of logs ($\\frac{A}{B} = \\exp (\\log A - \\log B )$), and we calculate this ratio as\n",
        "\n",
        "$$\n",
        "\\frac{\\pi_\\theta(a_t | s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t | s_t)} = \\exp \\left( \\log \\pi_\\theta(a_t | s_t) - \\log \\pi_{\\theta_{\\text{old}}}(a_t | s_t) \\right)\n",
        "$$\n",
        "\n",
        "\n",
        "*   Conceptually, defining policy loss as the product $\\frac{\\pi_\\theta(a_t | s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t | s_t)} \\cdot A_t$ is enough to train a policy. Feel free to try it and view your learning results in tensorboard.\n",
        "\n",
        "*   However, after several epochs of training, the new policy probabilities $\\pi_\\theta(a_t | s_t)$ may deviate so far from the old policy probabilities $\\pi_{\\theta_{\\text{old}}}(a_t | s_t)$, that the advantage data from the rollout (traj_data) is no longer valid. This can cause catastropic collapse in the policy.\n",
        "\n",
        "*   Enter $\\text{clip}\\left(\\frac{\\pi_\\theta(a_t | s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t | s_t)}, 1 - \\epsilon, 1 + \\epsilon\\right)$, which never lets the probability ratio become smaller than $1 - \\epsilon$ or larger than $1 + \\epsilon$, where common values of $\\epsilon$ are .2, .1, or .05. It's applied pointwise across all $(s, a)$ pairs.\n",
        "\n",
        "*   Finally, by taking the pointwise minimum of the unclipped and clipped products across all $(s, a)$ pairs, we ensure the largest policy update possible is made, while remaining conservatively close to the old policy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uozaYAZutDhk"
      },
      "outputs": [],
      "source": [
        "class Agent(nn.Module):\n",
        "    def __init__(self, n_obs, n_actions):\n",
        "        super().__init__()\n",
        "        self.name = 'PPO'\n",
        "\n",
        "        torch.manual_seed(0)  # needed before network init for fair comparison\n",
        "\n",
        "        # todo: student code here\n",
        "        self.policy = None  # replace\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        self.value = None  # replace\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # end student code\n",
        "\n",
        "    def get_loss(self, traj_data, epsilon=.1):\n",
        "\n",
        "        # todo: student code here\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        loss = None  # replace\n",
        "        # end student code\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def get_action(self, obs):\n",
        "        logits = self.policy(obs)\n",
        "        probs = categorical.Categorical(logits=logits)\n",
        "        actions = probs.sample()\n",
        "        return actions, probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdQ_b4e3shhy"
      },
      "outputs": [],
      "source": [
        "# @title PPO Unit Tests (must run PPO Agent cell above first)\n",
        "def PPO_networks():\n",
        "    a = Agent(12, 7)\n",
        "    assert a.name == 'PPO' and \\\n",
        "    a.policy(torch.randn(128, 12)).shape == (128, 7) and \\\n",
        "    a.value(torch.randn(4, 12)).shape == (4, 1) and \\\n",
        "    isinstance(list(a.policy.children())[-1], nn.Linear), \\\n",
        "    f\"Networks not initialized correctly\"\n",
        "    print(\"Test passed: PPO Networks appear correct!\")\n",
        "\n",
        "PPO_networks()\n",
        "\n",
        "def PPO_loss():\n",
        "    n_steps, n_envs, n_obs, n_actions = 10, 5, 4, 1\n",
        "    traj_data = TrajData(n_steps, n_envs, n_obs, n_actions)\n",
        "    torch.manual_seed(0)\n",
        "    traj_data.states = torch.rand_like(traj_data.states)\n",
        "    traj_data.actions = torch.randint(0, n_actions, traj_data.actions.shape)\n",
        "    traj_data.rewards = torch.rand_like(traj_data.rewards)\n",
        "    traj_data.not_dones = torch.randint(0, 2, traj_data.not_dones.shape)\n",
        "    traj_data.log_probs = torch.rand_like(traj_data.log_probs)\n",
        "    traj_data.returns = 2*torch.rand_like(traj_data.returns) - 1\n",
        "    a = Agent(4, 1)\n",
        "    torch.manual_seed(0)\n",
        "    a.policy = nn.Linear(4, 1)\n",
        "    a.value = nn.Linear(4, 1)\n",
        "    assert abs(a.get_loss(traj_data).item() - 0.9314) < 1e-4, \\\n",
        "    \"PPO loss does not match expected value.\"\n",
        "    print(\"Test passed: PPO loss appears correct!\")\n",
        "\n",
        "PPO_loss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkStaFnEMha7"
      },
      "source": [
        "Run the PPO cell above, and then run this cell, to plot results in tensorboard."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4yc9VZ0vyLd"
      },
      "outputs": [],
      "source": [
        "drl = DRL()\n",
        "for i in range(250):\n",
        "    drl.rollout(i)\n",
        "    drl.update()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-m4-XfO48lrk"
      },
      "source": [
        "#### PPO Conceptual Questions [ 0 pts / Ungraded / Not for credit]:\n",
        "Useful questions to check your understanding, **do not contribute to your grade** ...\n",
        "*   If advantage for a state-action pair $A(s_t, a_t)$ is a large positive number and epsilon is $\\epsilon = .2$, how might the probability ratio for $(s_t, a_t)$ evolve over 10 training epochs with a large learning rate? What if Advantage is large and negative? What if its zero? <br>\n",
        "\n",
        "\n",
        "*   When the clipped expression is activated for a given state-action pair, what is the gradient of the loss function with respect to network parameters for that state action pair? How will the probability of that action be changed during back propagation? $$\n",
        "\\text{clip}\\left(\\frac{\\pi_\\theta(a_t | s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t | s_t)}, 1 - \\epsilon, 1 + \\epsilon\\right)  \n",
        "$$\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUuZXJ_-Gc8q"
      },
      "source": [
        "## Optional Visulization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GmiAswbO6VbA"
      },
      "outputs": [],
      "source": [
        "untrained = DRL()\n",
        "visualize(untrained.agent)\n",
        "print(\"untrained agent\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iVZCJ1yBZwRe"
      },
      "outputs": [],
      "source": [
        "# each time this cell is run, a new random rollout is recorded\n",
        "# optional: put this cell below REINFORCE or VPG and re-run their training if youd like to visualize them.\n",
        "visualize(drl.agent)\n",
        "print(\"PPO trained agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3XTIdnpD0Xu"
      },
      "source": [
        "#### More Optional Extra Credit: GAE [4 pts]\n",
        "If you have time and you're up for a challenge... Implement Generalized Advantage Estimation for PPO  or VPG instead of our simple sum of discounted rewards. Plot the reward curves in tensorboard.\n",
        "\n",
        "This is a free form assignment. You must maintain the original functionality previously implemented, but besides that modify the code how ever you see fit. Copy-paste code below or modify it in place. For GAE, disregard these warnings: **You shouldnt need to change this code**. You will probably need to modify TrajData, DRL.rollout(), and implement a new Agent. Note, we havent tested how easy or hard these changes are, so proceed with caution.\n",
        "\n",
        "\n",
        "Here's a great lecture by Sergey Levine on [Eligability traces and GAE](https://www.youtube.com/watch?v=quRjnkj-MA0), especially starting from 7:41. Good luck and happy coding!\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
